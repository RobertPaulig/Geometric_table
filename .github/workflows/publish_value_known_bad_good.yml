name: publish-value-known-bad-good

permissions:
  actions: write
  contents: write
  pull-requests: write
  statuses: read

on:
  workflow_dispatch:
    inputs:
      ref:
        description: "Git ref to run on (branch, tag, or SHA). Example: main or 9c54b043..."
        required: true
        default: "main"
      tag:
        description: "Release tag to create/update. Must match: value-known-bad-good-YYYY-MM-DD(-rN)"
        required: true
      prerelease:
        description: "Mark GitHub release as pre-release"
        required: true
        default: "true"

jobs:
  build-packs:
    runs-on: ubuntu-latest
    outputs:
      source_sha: ${{ steps.meta.outputs.source_sha }}
      bad_constant_sha256: ${{ steps.hash_bad_constant.outputs.sha256 }}
      bad_random_sha256: ${{ steps.hash_bad_random.outputs.sha256 }}
      good_synthetic_sha256: ${{ steps.hash_good_synthetic.outputs.sha256 }}
      bad_constant_facts_md: ${{ steps.facts_bad_constant.outputs.facts_md }}
      bad_random_facts_md: ${{ steps.facts_bad_random.outputs.facts_md }}
      good_synthetic_facts_md: ${{ steps.facts_good_synthetic.outputs.facts_md }}
      asset_url_bad_constant: ${{ steps.meta.outputs.asset_url_bad_constant }}
      asset_url_bad_random: ${{ steps.meta.outputs.asset_url_bad_random }}
      asset_url_good_synthetic: ${{ steps.meta.outputs.asset_url_good_synthetic }}
      separation_md: ${{ steps.separation.outputs.separation_md }}

    steps:
      - uses: actions/checkout@v4
        with:
          ref: ${{ inputs.ref }}

      - name: Validate tag format
        run: |
          echo "${{ inputs.tag }}" | grep -E '^value-known-bad-good-[0-9]{4}-[0-9]{2}-[0-9]{2}(-r[0-9]+)?$'

      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: "pip"

      - name: Install (dev,chem)
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[dev,chem]"

      - name: Generate suite input (same generator+params as VALUE-M1)
        run: |
          python scripts/pilot_generate_input.py \
            --out_dir out_value_m2/suite \
            --rows 200 \
            --k_decoys 20 \
            --seed 0 \
            --full_cover_count 3

      - name: Generate 3 score variants (hetero_scores.v1)
        run: |
          python - << 'PY'
          import json
          import random
          from pathlib import Path
          
          base_path = Path("out_value_m2/suite/scores.json")
          base = json.loads(base_path.read_text(encoding="utf-8"))
          
          schema_version = base.get("schema_version", "")
          if schema_version != "hetero_scores.v1":
              raise ValueError(f"expected schema_version=hetero_scores.v1, got {schema_version!r}")
          
          def clone(obj):
              return json.loads(json.dumps(obj))
          
          # BAD-constant: all scores equal
          bad_constant = clone(base)
          bad_constant_score = 0.0
          bad_constant["original"]["score"] = bad_constant_score
          for decoy in bad_constant.get("decoys", {}).values():
              decoy["score"] = bad_constant_score
          
          # BAD-random: random scores with fixed seed
          bad_random = clone(base)
          rng = random.Random(0)
          bad_random["original"]["score"] = rng.random()
          for decoy in bad_random.get("decoys", {}).values():
              decoy["score"] = rng.random()
          
          # GOOD-synthetic: original higher than decoys
          good_synthetic = clone(base)
          good_synthetic["original"]["score"] = 1.0
          for decoy in good_synthetic.get("decoys", {}).values():
              decoy["score"] = 0.0
          
          out_dir = Path("out_value_m2/score_variants")
          out_dir.mkdir(parents=True, exist_ok=True)
          
          (out_dir / "scores_BAD-constant.json").write_text(json.dumps(bad_constant, indent=2, sort_keys=True), encoding="utf-8")
          (out_dir / "scores_BAD-random.json").write_text(json.dumps(bad_random, indent=2, sort_keys=True), encoding="utf-8")
          (out_dir / "scores_GOOD-synthetic.json").write_text(json.dumps(good_synthetic, indent=2, sort_keys=True), encoding="utf-8")
          PY

      - name: Run hetero2-batch (BAD-constant)
        run: |
          hetero2-batch \
            --input out_value_m2/suite/input.csv \
            --out_dir out_value_m2/BAD-constant \
            --artifacts light \
            --score_mode external_scores \
            --scores_input out_value_m2/score_variants/scores_BAD-constant.json \
            --k_decoys 20 \
            --workers 2 \
            --timeout_s 60 \
            --maxtasksperchild 100 \
            --seed_strategy per_row \
            --seed 0 \
            --zip_pack

      - name: Run hetero2-batch (BAD-random)
        run: |
          hetero2-batch \
            --input out_value_m2/suite/input.csv \
            --out_dir out_value_m2/BAD-random \
            --artifacts light \
            --score_mode external_scores \
            --scores_input out_value_m2/score_variants/scores_BAD-random.json \
            --k_decoys 20 \
            --workers 2 \
            --timeout_s 60 \
            --maxtasksperchild 100 \
            --seed_strategy per_row \
            --seed 0 \
            --zip_pack

      - name: Run hetero2-batch (GOOD-synthetic)
        run: |
          hetero2-batch \
            --input out_value_m2/suite/input.csv \
            --out_dir out_value_m2/GOOD-synthetic \
            --artifacts light \
            --score_mode external_scores \
            --scores_input out_value_m2/score_variants/scores_GOOD-synthetic.json \
            --k_decoys 20 \
            --workers 2 \
            --timeout_s 60 \
            --maxtasksperchild 100 \
            --seed_strategy per_row \
            --seed 0 \
            --zip_pack

      - name: Validate zip (no unpack) - BAD-constant
        run: |
          python -m zipfile -t out_value_m2/BAD-constant/evidence_pack.zip
          python -m zipfile -l out_value_m2/BAD-constant/evidence_pack.zip | grep -E "manifest.json|checksums.sha256|metrics.json|index.md|summary.csv"

      - name: Validate zip (no unpack) - BAD-random
        run: |
          python -m zipfile -t out_value_m2/BAD-random/evidence_pack.zip
          python -m zipfile -l out_value_m2/BAD-random/evidence_pack.zip | grep -E "manifest.json|checksums.sha256|metrics.json|index.md|summary.csv"

      - name: Validate zip (no unpack) - GOOD-synthetic
        run: |
          python -m zipfile -t out_value_m2/GOOD-synthetic/evidence_pack.zip
          python -m zipfile -l out_value_m2/GOOD-synthetic/evidence_pack.zip | grep -E "manifest.json|checksums.sha256|metrics.json|index.md|summary.csv"

      - name: "Gate: ERROR must be 0 (all variants)"
        run: |
          python - << 'PY'
          import json
          from pathlib import Path
          
          for variant in ["BAD-constant", "BAD-random", "GOOD-synthetic"]:
              metrics_path = Path(f"out_value_m2/{variant}/metrics.json")
              m = json.loads(metrics_path.read_text(encoding="utf-8"))
              err = int(m.get("counts", {}).get("ERROR", 0))
              assert err == 0, f"Gate-VALUE-M2 failed for {variant}: ERROR={err}"
              print(f"Gate OK: {variant} ERROR=0")
          PY

      - name: Prepare assets
        run: |
          cp out_value_m2/BAD-constant/evidence_pack.zip out_value_m2/value_known_bad_good_BAD-constant_evidence_pack.zip
          cp out_value_m2/BAD-random/evidence_pack.zip out_value_m2/value_known_bad_good_BAD-random_evidence_pack.zip
          cp out_value_m2/GOOD-synthetic/evidence_pack.zip out_value_m2/value_known_bad_good_GOOD-synthetic_evidence_pack.zip

      - name: Compute facts (summary.csv) - BAD-constant
        id: facts_bad_constant
        run: |
          python - << 'PY'
          import csv
          import os
          import statistics
          from collections import Counter
          from pathlib import Path
          
          summary_path = Path("out_value_m2/BAD-constant/summary.csv")
          rows = list(csv.DictReader(summary_path.read_text(encoding="utf-8").splitlines()))
          
          total = len(rows)
          status_counts = Counter((str(r.get("status", "")).strip() or "UNKNOWN") for r in rows)
          ok_rows = [r for r in rows if str(r.get("status", "")).strip() == "OK"]
          skip_reasons = Counter(
              (str(r.get("reason", "")).strip() or "UNKNOWN")
              for r in rows
              if str(r.get("status", "")).strip() == "SKIP" and str(r.get("reason", "")).strip()
          )
          top_skip = skip_reasons.most_common(5)
          
          rows_with_decoys = 0
          for r in rows:
              raw = str(r.get("n_decoys", "")).strip()
              try:
                  n_decoys = int(raw) if raw else 0
              except ValueError:
                  n_decoys = 0
              if n_decoys > 0:
                  rows_with_decoys += 1
          share = (rows_with_decoys / total) if total else 0.0
          
          if not ok_rows:
              raise AssertionError("no OK rows available to compute median(slack) / pass_rate")
          
          verdict_values = []
          slack_values = []
          for r in ok_rows:
              verdict = str(r.get("verdict", "")).strip()
              if not verdict:
                  raise AssertionError("missing required column/value on OK row: verdict")
              verdict_values.append(verdict)
              
              slack_raw = str(r.get("slack", "")).strip()
              if slack_raw == "":
                  raise AssertionError("missing required column/value on OK row: slack")
              try:
                  slack_values.append(float(slack_raw))
              except ValueError:
                  raise AssertionError(f"invalid slack value on OK row: {slack_raw!r}")
          
          def _nearest_rank_quantile(values_sorted: list[float], q: float) -> float:
              if not values_sorted:
                  return float("nan")
              if not (0.0 <= q <= 1.0):
                  raise AssertionError("q must be in [0,1]")
              k = max(1, int(round(q * len(values_sorted))))
              return float(values_sorted[min(k - 1, len(values_sorted) - 1)])
          
          pass_rate = sum(1 for v in verdict_values if v == "PASS") / len(verdict_values)
          mean_slack = statistics.mean(slack_values)
          slack_values_sorted = sorted(slack_values)
          p25_slack = _nearest_rank_quantile(slack_values_sorted, 0.25)
          median_slack = statistics.median(slack_values)
          p75_slack = _nearest_rank_quantile(slack_values_sorted, 0.75)
          
          lines = []
          lines.append(f"  - rows_total: {total}")
          lines.append(f"  - rows_ok: {len(ok_rows)}")
          lines.append(
              "  - status_counts: "
              f"OK={status_counts.get('OK', 0)}, SKIP={status_counts.get('SKIP', 0)}, ERROR={status_counts.get('ERROR', 0)}"
          )
          if top_skip:
              lines.append("  - top_skip_reasons:")
              for reason, cnt in top_skip:
                  lines.append(f"    - {reason}: {cnt}")
          else:
              lines.append("  - top_skip_reasons: (none)")
          lines.append(f"  - share_rows_with_n_decoys_gt_0: {share:.3f} ({share*100:.1f}%)")
          lines.append(f"  - mean_slack: {mean_slack:.6f}")
          lines.append(f"  - p25_slack: {p25_slack:.6f}")
          lines.append(f"  - median_slack: {median_slack:.6f}")
          lines.append(f"  - p75_slack: {p75_slack:.6f}")
          lines.append(f"  - pass_rate: {pass_rate:.6f}")
          
          facts_md = "\n".join(lines) + "\n"
          Path("out_value_m2/facts_BAD-constant.md").write_text(facts_md, encoding="utf-8")
          print(facts_md)
          
          gh_out = os.environ.get("GITHUB_OUTPUT")
          if gh_out:
              with open(gh_out, "a", encoding="utf-8") as f:
                  f.write("facts_md<<EOF\n")
                  f.write(facts_md)
                  f.write("EOF\n")
          PY

      - name: Compute facts (summary.csv) - BAD-random
        id: facts_bad_random
        run: |
          python - << 'PY'
          import csv
          import os
          import statistics
          from collections import Counter
          from pathlib import Path
          
          summary_path = Path("out_value_m2/BAD-random/summary.csv")
          rows = list(csv.DictReader(summary_path.read_text(encoding="utf-8").splitlines()))
          
          total = len(rows)
          status_counts = Counter((str(r.get("status", "")).strip() or "UNKNOWN") for r in rows)
          ok_rows = [r for r in rows if str(r.get("status", "")).strip() == "OK"]
          skip_reasons = Counter(
              (str(r.get("reason", "")).strip() or "UNKNOWN")
              for r in rows
              if str(r.get("status", "")).strip() == "SKIP" and str(r.get("reason", "")).strip()
          )
          top_skip = skip_reasons.most_common(5)
          
          rows_with_decoys = 0
          for r in rows:
              raw = str(r.get("n_decoys", "")).strip()
              try:
                  n_decoys = int(raw) if raw else 0
              except ValueError:
                  n_decoys = 0
              if n_decoys > 0:
                  rows_with_decoys += 1
          share = (rows_with_decoys / total) if total else 0.0
          
          if not ok_rows:
              raise AssertionError("no OK rows available to compute median(slack) / pass_rate")
          
          verdict_values = []
          slack_values = []
          for r in ok_rows:
              verdict = str(r.get("verdict", "")).strip()
              if not verdict:
                  raise AssertionError("missing required column/value on OK row: verdict")
              verdict_values.append(verdict)
              
              slack_raw = str(r.get("slack", "")).strip()
              if slack_raw == "":
                  raise AssertionError("missing required column/value on OK row: slack")
              try:
                  slack_values.append(float(slack_raw))
              except ValueError:
                  raise AssertionError(f"invalid slack value on OK row: {slack_raw!r}")
          
          def _nearest_rank_quantile(values_sorted: list[float], q: float) -> float:
              if not values_sorted:
                  return float("nan")
              if not (0.0 <= q <= 1.0):
                  raise AssertionError("q must be in [0,1]")
              k = max(1, int(round(q * len(values_sorted))))
              return float(values_sorted[min(k - 1, len(values_sorted) - 1)])
          
          pass_rate = sum(1 for v in verdict_values if v == "PASS") / len(verdict_values)
          mean_slack = statistics.mean(slack_values)
          slack_values_sorted = sorted(slack_values)
          p25_slack = _nearest_rank_quantile(slack_values_sorted, 0.25)
          median_slack = statistics.median(slack_values)
          p75_slack = _nearest_rank_quantile(slack_values_sorted, 0.75)
          
          lines = []
          lines.append(f"  - rows_total: {total}")
          lines.append(f"  - rows_ok: {len(ok_rows)}")
          lines.append(
              "  - status_counts: "
              f"OK={status_counts.get('OK', 0)}, SKIP={status_counts.get('SKIP', 0)}, ERROR={status_counts.get('ERROR', 0)}"
          )
          if top_skip:
              lines.append("  - top_skip_reasons:")
              for reason, cnt in top_skip:
                  lines.append(f"    - {reason}: {cnt}")
          else:
              lines.append("  - top_skip_reasons: (none)")
          lines.append(f"  - share_rows_with_n_decoys_gt_0: {share:.3f} ({share*100:.1f}%)")
          lines.append(f"  - mean_slack: {mean_slack:.6f}")
          lines.append(f"  - p25_slack: {p25_slack:.6f}")
          lines.append(f"  - median_slack: {median_slack:.6f}")
          lines.append(f"  - p75_slack: {p75_slack:.6f}")
          lines.append(f"  - pass_rate: {pass_rate:.6f}")
          
          facts_md = "\n".join(lines) + "\n"
          Path("out_value_m2/facts_BAD-random.md").write_text(facts_md, encoding="utf-8")
          print(facts_md)
          
          gh_out = os.environ.get("GITHUB_OUTPUT")
          if gh_out:
              with open(gh_out, "a", encoding="utf-8") as f:
                  f.write("facts_md<<EOF\n")
                  f.write(facts_md)
                  f.write("EOF\n")
          PY

      - name: Compute facts (summary.csv) - GOOD-synthetic
        id: facts_good_synthetic
        run: |
          python - << 'PY'
          import csv
          import os
          import statistics
          from collections import Counter
          from pathlib import Path
          
          summary_path = Path("out_value_m2/GOOD-synthetic/summary.csv")
          rows = list(csv.DictReader(summary_path.read_text(encoding="utf-8").splitlines()))
          
          total = len(rows)
          status_counts = Counter((str(r.get("status", "")).strip() or "UNKNOWN") for r in rows)
          ok_rows = [r for r in rows if str(r.get("status", "")).strip() == "OK"]
          skip_reasons = Counter(
              (str(r.get("reason", "")).strip() or "UNKNOWN")
              for r in rows
              if str(r.get("status", "")).strip() == "SKIP" and str(r.get("reason", "")).strip()
          )
          top_skip = skip_reasons.most_common(5)
          
          rows_with_decoys = 0
          for r in rows:
              raw = str(r.get("n_decoys", "")).strip()
              try:
                  n_decoys = int(raw) if raw else 0
              except ValueError:
                  n_decoys = 0
              if n_decoys > 0:
                  rows_with_decoys += 1
          share = (rows_with_decoys / total) if total else 0.0
          
          if not ok_rows:
              raise AssertionError("no OK rows available to compute median(slack) / pass_rate")
          
          verdict_values = []
          slack_values = []
          for r in ok_rows:
              verdict = str(r.get("verdict", "")).strip()
              if not verdict:
                  raise AssertionError("missing required column/value on OK row: verdict")
              verdict_values.append(verdict)
              
              slack_raw = str(r.get("slack", "")).strip()
              if slack_raw == "":
                  raise AssertionError("missing required column/value on OK row: slack")
              try:
                  slack_values.append(float(slack_raw))
              except ValueError:
                  raise AssertionError(f"invalid slack value on OK row: {slack_raw!r}")
          
          def _nearest_rank_quantile(values_sorted: list[float], q: float) -> float:
              if not values_sorted:
                  return float("nan")
              if not (0.0 <= q <= 1.0):
                  raise AssertionError("q must be in [0,1]")
              k = max(1, int(round(q * len(values_sorted))))
              return float(values_sorted[min(k - 1, len(values_sorted) - 1)])
          
          pass_rate = sum(1 for v in verdict_values if v == "PASS") / len(verdict_values)
          mean_slack = statistics.mean(slack_values)
          slack_values_sorted = sorted(slack_values)
          p25_slack = _nearest_rank_quantile(slack_values_sorted, 0.25)
          median_slack = statistics.median(slack_values)
          p75_slack = _nearest_rank_quantile(slack_values_sorted, 0.75)
          
          lines = []
          lines.append(f"  - rows_total: {total}")
          lines.append(f"  - rows_ok: {len(ok_rows)}")
          lines.append(
              "  - status_counts: "
              f"OK={status_counts.get('OK', 0)}, SKIP={status_counts.get('SKIP', 0)}, ERROR={status_counts.get('ERROR', 0)}"
          )
          if top_skip:
              lines.append("  - top_skip_reasons:")
              for reason, cnt in top_skip:
                  lines.append(f"    - {reason}: {cnt}")
          else:
              lines.append("  - top_skip_reasons: (none)")
          lines.append(f"  - share_rows_with_n_decoys_gt_0: {share:.3f} ({share*100:.1f}%)")
          lines.append(f"  - mean_slack: {mean_slack:.6f}")
          lines.append(f"  - p25_slack: {p25_slack:.6f}")
          lines.append(f"  - median_slack: {median_slack:.6f}")
          lines.append(f"  - p75_slack: {p75_slack:.6f}")
          lines.append(f"  - pass_rate: {pass_rate:.6f}")
          
          facts_md = "\n".join(lines) + "\n"
          Path("out_value_m2/facts_GOOD-synthetic.md").write_text(facts_md, encoding="utf-8")
          print(facts_md)
          
          gh_out = os.environ.get("GITHUB_OUTPUT")
          if gh_out:
              with open(gh_out, "a", encoding="utf-8") as f:
                  f.write("facts_md<<EOF\n")
                  f.write(facts_md)
                  f.write("EOF\n")
          PY

      - name: Compute separation facts (Δ vs BAD-constant; OK-only)
        id: separation
        run: |
          python - << 'PY'
          import csv
          import os
          import statistics
          from pathlib import Path
          
          def _nearest_rank_quantile(values_sorted: list[float], q: float) -> float:
              if not values_sorted:
                  return float("nan")
              if not (0.0 <= q <= 1.0):
                  raise AssertionError("q must be in [0,1]")
              k = max(1, int(round(q * len(values_sorted))))
              return float(values_sorted[min(k - 1, len(values_sorted) - 1)])
          
          def read_ok_metrics(summary_csv: Path) -> dict[str, object]:
              rows = list(csv.DictReader(summary_csv.read_text(encoding="utf-8").splitlines()))
              ok_rows = [r for r in rows if str(r.get("status", "")).strip() == "OK"]
              if not ok_rows:
                  raise AssertionError(f"no OK rows in {summary_csv}")
              
              slack_values = []
              verdict_values = []
              for r in ok_rows:
                  slack_raw = str(r.get("slack", "")).strip()
                  if slack_raw == "":
                      raise AssertionError(f"missing slack on OK row in {summary_csv}")
                  slack_values.append(float(slack_raw))
                  
                  verdict = str(r.get("verdict", "")).strip()
                  if not verdict:
                      raise AssertionError(f"missing verdict on OK row in {summary_csv}")
                  verdict_values.append(verdict)
              
              slack_values_sorted = sorted(slack_values)
              median_slack = statistics.median(slack_values)
              mean_slack = statistics.mean(slack_values)
              p25_slack = _nearest_rank_quantile(slack_values_sorted, 0.25)
              p75_slack = _nearest_rank_quantile(slack_values_sorted, 0.75)
              pass_rate = sum(1 for v in verdict_values if v == "PASS") / len(verdict_values)
              return {
                  "rows_ok": len(ok_rows),
                  "rows_total": len(rows),
                  "mean_slack": mean_slack,
                  "p25_slack": p25_slack,
                  "median_slack": median_slack,
                  "p75_slack": p75_slack,
                  "pass_rate": pass_rate,
              }
          
          bad = read_ok_metrics(Path("out_value_m2/BAD-constant/summary.csv"))
          rnd = read_ok_metrics(Path("out_value_m2/BAD-random/summary.csv"))
          good = read_ok_metrics(Path("out_value_m2/GOOD-synthetic/summary.csv"))
          
          lines = []
          lines.append("separation facts (computed on status==OK rows only):")
          lines.append(
              "- BAD-constant: "
              f"rows_ok={bad['rows_ok']}/{bad['rows_total']}, "
              f"mean_slack={bad['mean_slack']:.6f}, p25_slack={bad['p25_slack']:.6f}, median_slack={bad['median_slack']:.6f}, p75_slack={bad['p75_slack']:.6f}, "
              f"pass_rate={bad['pass_rate']:.6f}"
          )
          lines.append(
              "- BAD-random: "
              f"rows_ok={rnd['rows_ok']}/{rnd['rows_total']}, "
              f"mean_slack={rnd['mean_slack']:.6f}, p25_slack={rnd['p25_slack']:.6f}, median_slack={rnd['median_slack']:.6f}, p75_slack={rnd['p75_slack']:.6f}, "
              f"pass_rate={rnd['pass_rate']:.6f}"
          )
          lines.append(
              "- GOOD-synthetic: "
              f"rows_ok={good['rows_ok']}/{good['rows_total']}, "
              f"mean_slack={good['mean_slack']:.6f}, p25_slack={good['p25_slack']:.6f}, median_slack={good['median_slack']:.6f}, p75_slack={good['p75_slack']:.6f}, "
              f"pass_rate={good['pass_rate']:.6f}"
          )
          lines.append("")
          lines.append(f"- Δ_mean_slack(GOOD - BAD-constant): {(good['mean_slack'] - bad['mean_slack']):.6f}")
          lines.append(f"- Δ_p25_slack(GOOD - BAD-constant): {(good['p25_slack'] - bad['p25_slack']):.6f}")
          lines.append(f"- Δ_median_slack(GOOD - BAD-constant): {(good['median_slack'] - bad['median_slack']):.6f}")
          lines.append(f"- Δ_p75_slack(GOOD - BAD-constant): {(good['p75_slack'] - bad['p75_slack']):.6f}")
          lines.append(f"- Δ_PASS_rate(GOOD - BAD-constant): {(good['pass_rate'] - bad['pass_rate']):.6f}")
          lines.append("")
          lines.append(f"- Δ_mean_slack(GOOD - BAD-random): {(good['mean_slack'] - rnd['mean_slack']):.6f}")
          lines.append(f"- Δ_p25_slack(GOOD - BAD-random): {(good['p25_slack'] - rnd['p25_slack']):.6f}")
          lines.append(f"- Δ_median_slack(GOOD - BAD-random): {(good['median_slack'] - rnd['median_slack']):.6f}")
          lines.append(f"- Δ_p75_slack(GOOD - BAD-random): {(good['p75_slack'] - rnd['p75_slack']):.6f}")
          lines.append(f"- Δ_PASS_rate(GOOD - BAD-random): {(good['pass_rate'] - rnd['pass_rate']):.6f}")
          
          separation_md = "\n".join(lines) + "\n"
          Path("out_value_m2/separation_facts.md").write_text(separation_md, encoding="utf-8")
          print(separation_md)
          
          gh_out = os.environ.get("GITHUB_OUTPUT")
          if gh_out:
              with open(gh_out, "a", encoding="utf-8") as f:
                  f.write("separation_md<<EOF\n")
                  f.write(separation_md)
                  f.write("EOF\n")
          PY

      - name: Compute SHA256 - BAD-constant
        id: hash_bad_constant
        run: |
          SHA256=$(sha256sum out_value_m2/value_known_bad_good_BAD-constant_evidence_pack.zip | awk '{print toupper($1)}')
          echo "sha256=$SHA256" >> "$GITHUB_OUTPUT"
          echo "$SHA256  value_known_bad_good_BAD-constant_evidence_pack.zip" > out_value_m2/value_known_bad_good_BAD-constant_evidence_pack.zip.sha256

      - name: Compute SHA256 - BAD-random
        id: hash_bad_random
        run: |
          SHA256=$(sha256sum out_value_m2/value_known_bad_good_BAD-random_evidence_pack.zip | awk '{print toupper($1)}')
          echo "sha256=$SHA256" >> "$GITHUB_OUTPUT"
          echo "$SHA256  value_known_bad_good_BAD-random_evidence_pack.zip" > out_value_m2/value_known_bad_good_BAD-random_evidence_pack.zip.sha256

      - name: Compute SHA256 - GOOD-synthetic
        id: hash_good_synthetic
        run: |
          SHA256=$(sha256sum out_value_m2/value_known_bad_good_GOOD-synthetic_evidence_pack.zip | awk '{print toupper($1)}')
          echo "sha256=$SHA256" >> "$GITHUB_OUTPUT"
          echo "$SHA256  value_known_bad_good_GOOD-synthetic_evidence_pack.zip" > out_value_m2/value_known_bad_good_GOOD-synthetic_evidence_pack.zip.sha256

      - name: Write release notes
        run: |
          cat > release_notes.md << 'EOF'
          VALUE-M2 known bad / good evidence packs (light artifacts, external scores)

          - Source ref: ${{ inputs.ref }}
          - Input generation (suite):
            python scripts/pilot_generate_input.py --out_dir out_value_m2/suite --rows 200 --k_decoys 20 --seed 0 --full_cover_count 3
          - Batch (repro, each variant):
            hetero2-batch --input out_value_m2/suite/input.csv --out_dir out_value_m2/<VARIANT> --artifacts light --score_mode external_scores --scores_input out_value_m2/score_variants/<SCORES>.json --k_decoys 20 --workers 2 --timeout_s 60 --maxtasksperchild 100 --seed_strategy per_row --seed 0 --zip_pack

          Assets:
          - value_known_bad_good_BAD-constant_evidence_pack.zip
          - value_known_bad_good_BAD-random_evidence_pack.zip
          - value_known_bad_good_GOOD-synthetic_evidence_pack.zip

          Facts (BAD-constant):
          - SHA256: ${{ steps.hash_bad_constant.outputs.sha256 }}
          ${{ steps.facts_bad_constant.outputs.facts_md }}

          Facts (BAD-random):
          - SHA256: ${{ steps.hash_bad_random.outputs.sha256 }}
          ${{ steps.facts_bad_random.outputs.facts_md }}

          Facts (GOOD-synthetic):
          - SHA256: ${{ steps.hash_good_synthetic.outputs.sha256 }}
          ${{ steps.facts_good_synthetic.outputs.facts_md }}

          Separation facts (OK-only; no auto-threshold gating):
          ${{ steps.separation.outputs.separation_md }}
          EOF

      - name: Prepare meta (asset URLs, source SHA)
        id: meta
        run: |
          SOURCE_SHA=$(git rev-parse HEAD)
          echo "source_sha=$SOURCE_SHA" >> "$GITHUB_OUTPUT"
          echo "asset_url_bad_constant=https://github.com/${{ github.repository }}/releases/download/${{ inputs.tag }}/value_known_bad_good_BAD-constant_evidence_pack.zip" >> "$GITHUB_OUTPUT"
          echo "asset_url_bad_random=https://github.com/${{ github.repository }}/releases/download/${{ inputs.tag }}/value_known_bad_good_BAD-random_evidence_pack.zip" >> "$GITHUB_OUTPUT"
          echo "asset_url_good_synthetic=https://github.com/${{ github.repository }}/releases/download/${{ inputs.tag }}/value_known_bad_good_GOOD-synthetic_evidence_pack.zip" >> "$GITHUB_OUTPUT"

      - name: Upload build artifact(s)
        uses: actions/upload-artifact@v4
        with:
          name: value-known-bad-good-build
          path: |
            out_value_m2/value_known_bad_good_BAD-constant_evidence_pack.zip
            out_value_m2/value_known_bad_good_BAD-constant_evidence_pack.zip.sha256
            out_value_m2/value_known_bad_good_BAD-random_evidence_pack.zip
            out_value_m2/value_known_bad_good_BAD-random_evidence_pack.zip.sha256
            out_value_m2/value_known_bad_good_GOOD-synthetic_evidence_pack.zip
            out_value_m2/value_known_bad_good_GOOD-synthetic_evidence_pack.zip.sha256
            out_value_m2/facts_BAD-constant.md
            out_value_m2/facts_BAD-random.md
            out_value_m2/facts_GOOD-synthetic.md
            out_value_m2/separation_facts.md
            release_notes.md

  publish-release:
    needs: build-packs
    runs-on: ubuntu-latest

    steps:
      - uses: actions/download-artifact@v4
        with:
          name: value-known-bad-good-build
          path: .

      - name: "Gate: required CI contexts must be success"
        env:
          SOURCE_SHA: ${{ needs.build-packs.outputs.source_sha }}
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          python - << 'PY'
          import json
          import os
          import sys
          import urllib.request
          
          repo = os.environ["GITHUB_REPOSITORY"]
          sha = os.environ["SOURCE_SHA"]
          token = os.environ["GITHUB_TOKEN"]
          
          url = f"https://api.github.com/repos/{repo}/commits/{sha}/status"
          req = urllib.request.Request(
              url,
              headers={
                  "Authorization": f"token {token}",
                  "Accept": "application/vnd.github+json",
              },
          )
          with urllib.request.urlopen(req) as resp:
              data = json.load(resp)
          
          latest_by_context = {}
          for st in data.get("statuses", []):
              ctx = st.get("context", "")
              if ctx and ctx not in latest_by_context:
                  latest_by_context[ctx] = st.get("state", "")
          
          required = ["ci/test", "ci/test-chem", "ci/docker"]
          missing = [c for c in required if c not in latest_by_context]
          bad = [c for c in required if latest_by_context.get(c) != "success"]
          
          if missing or bad:
              print("Gate-Required-Contexts failed")
              print(f"source_sha={sha}")
              print(f"missing={missing}")
              print("states=" + json.dumps({c: latest_by_context.get(c) for c in required}, indent=2, sort_keys=True))
              sys.exit(1)
          
          print("Gate OK: required contexts success")
          print(f"source_sha={sha}")
          for c in required:
              print(f"{c}=success")
          PY

      - name: Create/Update GitHub Release + upload assets
        uses: softprops/action-gh-release@v2
        with:
          tag_name: ${{ inputs.tag }}
          target_commitish: ${{ needs.build-packs.outputs.source_sha }}
          name: "VALUE-M2 known bad/good evidence packs (ERROR=0)"
          body_path: release_notes.md
          prerelease: ${{ inputs.prerelease }}
          files: |
            out_value_m2/value_known_bad_good_BAD-constant_evidence_pack.zip
            out_value_m2/value_known_bad_good_BAD-constant_evidence_pack.zip.sha256
            out_value_m2/value_known_bad_good_BAD-random_evidence_pack.zip
            out_value_m2/value_known_bad_good_BAD-random_evidence_pack.zip.sha256
            out_value_m2/value_known_bad_good_GOOD-synthetic_evidence_pack.zip
            out_value_m2/value_known_bad_good_GOOD-synthetic_evidence_pack.zip.sha256
          fail_on_unmatched_files: true
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

  registry-pr:
    needs: [build-packs, publish-release]
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4
        with:
          ref: "main"

      - name: Update artefacts registry (append)
        run: |
          mkdir -p docs
          FILE="docs/artefacts_registry.md"
          if [ ! -f "$FILE" ]; then
            echo "# Artefacts Registry" > "$FILE"
            echo "" >> "$FILE"
          fi

          cat >> "$FILE" << EOF

          ## ${{ inputs.tag }}

          - Source commit: ${{ needs.build-packs.outputs.source_sha }}

          - Variant: BAD-constant
            - Release asset: ${{ needs.build-packs.outputs.asset_url_bad_constant }}
            - SHA256(value_known_bad_good_BAD-constant_evidence_pack.zip): ${{ needs.build-packs.outputs.bad_constant_sha256 }}
            - Command:
              python scripts/pilot_generate_input.py --out_dir out_value_m2/suite --rows 200 --k_decoys 20 --seed 0 --full_cover_count 3
              (scores variant) BAD-constant: all scores equal
              hetero2-batch --input out_value_m2/suite/input.csv --out_dir out_value_m2/BAD-constant --artifacts light --score_mode external_scores --scores_input out_value_m2/score_variants/scores_BAD-constant.json --k_decoys 20 --workers 2 --timeout_s 60 --maxtasksperchild 100 --seed_strategy per_row --seed 0 --zip_pack
            - Outcome (facts from summary.csv):
          ${{ needs.build-packs.outputs.bad_constant_facts_md }}

          - Variant: BAD-random
            - Release asset: ${{ needs.build-packs.outputs.asset_url_bad_random }}
            - SHA256(value_known_bad_good_BAD-random_evidence_pack.zip): ${{ needs.build-packs.outputs.bad_random_sha256 }}
            - Command:
              python scripts/pilot_generate_input.py --out_dir out_value_m2/suite --rows 200 --k_decoys 20 --seed 0 --full_cover_count 3
              (scores variant) BAD-random: random scores (seed=0)
              hetero2-batch --input out_value_m2/suite/input.csv --out_dir out_value_m2/BAD-random --artifacts light --score_mode external_scores --scores_input out_value_m2/score_variants/scores_BAD-random.json --k_decoys 20 --workers 2 --timeout_s 60 --maxtasksperchild 100 --seed_strategy per_row --seed 0 --zip_pack
            - Outcome (facts from summary.csv):
          ${{ needs.build-packs.outputs.bad_random_facts_md }}

          - Variant: GOOD-synthetic
            - Release asset: ${{ needs.build-packs.outputs.asset_url_good_synthetic }}
            - SHA256(value_known_bad_good_GOOD-synthetic_evidence_pack.zip): ${{ needs.build-packs.outputs.good_synthetic_sha256 }}
            - Command:
              python scripts/pilot_generate_input.py --out_dir out_value_m2/suite --rows 200 --k_decoys 20 --seed 0 --full_cover_count 3
              (scores variant) GOOD-synthetic: original=1.0, decoys=0.0
              hetero2-batch --input out_value_m2/suite/input.csv --out_dir out_value_m2/GOOD-synthetic --artifacts light --score_mode external_scores --scores_input out_value_m2/score_variants/scores_GOOD-synthetic.json --k_decoys 20 --workers 2 --timeout_s 60 --maxtasksperchild 100 --seed_strategy per_row --seed 0 --zip_pack
            - Outcome (facts from summary.csv):
          ${{ needs.build-packs.outputs.good_synthetic_facts_md }}

          - Separation facts (OK-only; no auto-threshold gating):
          ${{ needs.build-packs.outputs.separation_md }}
          EOF

      - name: Create PR with registry update
        id: registry_cpr
        uses: peter-evans/create-pull-request@v6
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          branch: "automation/artefact-${{ inputs.tag }}"
          title: "Add artefact registry entry for ${{ inputs.tag }}"
          commit-message: "docs: add artefact registry entry for ${{ inputs.tag }}"
          body: |
            Automated update: appended artefact entries after publishing VALUE-M2 known bad/good release assets.
          base: "main"
          add-paths: |
            docs/artefacts_registry.md

      - name: Trigger CI (pytest workflow_dispatch) for automation PR branch
        if: steps.registry_cpr.outputs.pull-request-url != ''
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          REPO: ${{ github.repository }}
          BRANCH: "automation/artefact-${{ inputs.tag }}"
          PR_URL: ${{ steps.registry_cpr.outputs.pull-request-url }}
        run: |
          set -euo pipefail
          echo "Automation PR: $PR_URL"
          echo "Triggering pytest workflow on branch: $BRANCH"

          gh workflow run pytest.yml --ref "$BRANCH" --repo "$REPO"

          RUN_ID=""
          for i in $(seq 1 30); do
            RUN_ID=$(gh run list --workflow pytest.yml --branch "$BRANCH" --event workflow_dispatch --limit 1 --json databaseId --jq '.[0].databaseId' --repo "$REPO" 2>/dev/null || true)
            if [ -n "$RUN_ID" ] && [ "$RUN_ID" != "null" ]; then
              break
            fi
            sleep 2
          done

          if [ -z "$RUN_ID" ] || [ "$RUN_ID" = "null" ]; then
            echo "ERROR: failed to locate dispatched pytest run"
            gh run list --workflow pytest.yml --branch "$BRANCH" --limit 5 --repo "$REPO" || true
            exit 1
          fi

          RUN_URL=$(gh run view "$RUN_ID" --json url --jq '.url' --repo "$REPO")
          TESTED_SHA=$(gh run view "$RUN_ID" --json headSha --jq '.headSha' --repo "$REPO")
          echo "pytest run url: $RUN_URL"
          echo "tested_sha: $TESTED_SHA"

          gh run watch "$RUN_ID" --exit-status --repo "$REPO"
