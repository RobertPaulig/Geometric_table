import numpy as np
from dataclasses import dataclass
from typing import List, Callable, Optional, Sequence

@dataclass
class IFS:
    """
    Iterated Function System (IFS).
    Defined by a set of contractive maps w_i: R^d -> R^d.
    """
    maps: Sequence[Callable[[np.ndarray], np.ndarray]]
    probabilities: Optional[Sequence[float]] = None

    def __post_init__(self):
        if self.probabilities is None:
            n = len(self.maps)
            self.probabilities = [1.0 / n] * n
        else:
            # Normalize
            s = sum(self.probabilities)
            self.probabilities = [p / s for p in self.probabilities]

    def generate_points(self, depth: int, x0: Optional[np.ndarray] = None) -> np.ndarray:
        """
        Generate points in the attractor (or approximation) using chaos game
        or deterministic tree traversal.
        
        For integration purposes, we often use a deterministic full tree expansion
        up to depth K to cover the measure uniformly.
        
        Here we implement a tensor-product style generation for 'make_tensor_grid_ifs',
        or a general random walk if random=True.
        
        For FDM integration, we typically enumerate all N = m^depth branches.
        """
        # For now, let's implement the deterministic enumeration for small depths
        # which is ideal for FDM integration on grids.
        
        # Base case: depth 0 -> single point (center or x0)
        # But wait, for integration we usually start from the whole domain.
        # Let's assume this is strictly for generating sample points X_k.
        
        # Simple recursion for all branches for moderate depth
        # Warning: N grows as len(maps)^depth.
        
        points = []
        if x0 is None:
            # Default start point (dummy, often 0 or 0.5)
            # We assume maps handle 1D arrays or batches.
            # Let's try to infer dim from maps[0] if possible, or just pass 0.
            pass

        # Optimized vectorized approach for "tensor grid" style maps would be better,
        # but for a general generic IFS, we have to iterate.
        
        # Let's assume maps can handle batch input: (N, dim) -> (N, dim)
        pass
        
        return np.array([]) # Placeholder, see FDMIntegrator below


class FDMIntegrator:
    """
    Integrator using Fractal Decomposition Method.
    Approximates Integral(f dmu) where mu is the invariant measure of the IFS.
    """
    def __init__(self, ifs: IFS):
        self.ifs = ifs

    def sample(self, depth: int, dim: int, x0: Optional[np.ndarray] = None) -> np.ndarray:
        """
        Generate a sample cloud covering the attractor.
        For a tensor grid IFS on [0,1]^d, this generates the centers (or corners)
        of the subsquares at level 'depth'.
        
        Returns: (N_points, dim) array.
        """
        m = len(self.ifs.maps)
        num_points = m ** depth
        
        if x0 is None:
            # Start from the center of [0,1]^dim?
            # Or better, start with a single point 0.5 and apply all maps iteratively.
            current_points = np.zeros((1, dim)) + 0.5
        else:
            current_points = x0.reshape(1, -1)
            
        for _ in range(depth):
            # Apply all maps to all current points
            new_points_list = []
            for w in self.ifs.maps:
                # w should support batch (N, dim) -> (N, dim)
                # If not, we map row by row.
                # Let's assume our maps are simple functions.
                mapped = w(current_points) 
                new_points_list.append(mapped)
            
            current_points = np.vstack(new_points_list)
            
        return current_points

    def integrate(
        self, 
        f: Callable[[np.ndarray], np.ndarray], 
        depth: int, 
        dim: int, 
        transform: Optional[Callable[[np.ndarray], np.ndarray]] = None,
        vectorized: bool = True
    ) -> float:
        """
        Compute integral Estimate = Mean(f(x_i)).
        
        x_i are generated by the IFS.
        If transform is provided, x_i' = transform(x_i) before passing to f.
        This allows mapping [0,1]^3 -> Ball, etc.
        """
        # 1. Generate sample points in the "code space" / domain U (e.g. [0,1]^d)
        u = self.sample(depth, dim)
        
        # 2. Transform domain if needed
        if transform is not None:
            x = transform(u)
        else:
            x = u
            
        # 3. Evaluate function
        if vectorized:
            values = f(x)
        else:
            values = np.array([f(pt) for pt in x])
            
        # 4. Average
        # Note: this assumes uniform weights (probabilities=1/m).
        # If IFS has non-uniform probs, we need to apply weights. 
        # For this version, we assume tensor grid with equal weights.
        return float(np.mean(values))


def make_tensor_grid_ifs(dim: int, base: int = 2) -> IFS:
    """
    Constructs an IFS whose attractor is [0,1]^dim with Lebesgue measure.
    Uses 'base' subdivisions along each axis.
    Total maps: base^dim.
    
    Each map is w(x) = (x + shift) / base.
    """
    import itertools
    
    # Generate all shift vectors from {0, ..., base-1}^dim
    shifts = list(itertools.product(range(base), repeat=dim))
    
    maps = []
    
    # Define closure for the map
    def make_map(s_vec):
        shift = np.array(s_vec, dtype=float)
        return lambda x: (x + shift) / base

    for s in shifts:
        maps.append(make_map(s))
        
    return IFS(maps=maps)


if __name__ == "__main__":
    print("=== FDM Engine Self-Test ===")
    
    # Test 1: Integrate f(x,y) = x^2 + y^2 over [0,1]^2
    # Exact value: Integral_0^1 dx Integral_0^1 dy (x^2 + y^2) = 1/3 + 1/3 = 2/3 â‰ˆ 0.666666...
    
    dim = 2
    ifs = make_tensor_grid_ifs(dim=dim, base=2)
    fdm = FDMIntegrator(ifs)
    
    def test_func(p):
        # p is (N, 2)
        x = p[:, 0]
        y = p[:, 1]
        return x**2 + y**2
    
    print(f"Target Integral Value: {2/3:.6f}")
    
    for k in range(1, 8):
        # depth k
        val = fdm.integrate(test_func, depth=k, dim=dim)
        n_pts = (2**dim)**k
        err = abs(val - 2.0/3.0)
        print(f"Depth {k}: N={n_pts:<6}  I ~ {val:.6f}  (Err: {err:.2e})")
        
    print("=== Done ===")
